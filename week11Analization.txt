Hash array size 3 (NanoSeconds)
	Abel = 13,416
	Cobbs = 123,041
	Harlow = 205,531
	Savage = 333,875
	Zuniga = 755,167
	non-existing user = 496,416

Hash array size 10 (NanoSeconds)
	Abel = 49,750
	Cobbs = 86,250
	Harlow = 88,750
	Savage = 119,541
	Young = 257,583 
	non-existing user = 161,792

Hash array size 100 (NanoSeconds)
	Abel = 15,542
	Cobbs = 46,667
	Harlow = 57,000
	Savage = 62,125
	Wicks = 90,583 
	non-existing user = 77,834

Hash array size 500 (NanoSeconds)
	Abel = 59,583
	Cobbs = 57,042
	Harlow = 54,917
	Savage = 66,875
	Zarate = 76,500 
	non-existing user = 62,250

Hash array size 1000 (NanoSeconds)
	Abel = 64,083
	Cobbs = 66,083
	Harlow = 91,708
	Savage = 65,916
	Zarate = 69,667
	non-existing user =  43,625

Hash array size 4000 (NanoSeconds)
	Abel = 65,458
	Cobbs = 71,042
	Harlow = 79,417
	Savage = 84,958
	Zuniga = 78,541
	non-existing user = 68,542

I believe the optimal size for the list size of 4,500 for optimal time would be between 500 and 100 so about 300 due to the speeds getting faster up to 100 and slowing down to 500.

Conjecture: If the data set was much larger the hash size would need to be larger to have decent time but it would be slow as the different hash sizes donâ€™t make a large difference in speed.